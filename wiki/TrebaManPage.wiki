#labels Featured
<pre>
treba(1)                                                              treba(1)




</pre>
<h2>NAME</h2><pre>
       treba - probabilistic finite-state automaton training and decoding


</pre>
<h2>SYNOPSIS</h2><pre>
       treba [options] [ OBSERVATIONS-FILE ]


</pre>
<h2>DESCRIPTION</h2><pre>
       treba  is  a tool for training, decoding, and calculating with weighted
       (probabilistic) finite state  automata.   Training  algorithms  include
       Baum-Welch (EM), Viterbi training, and Baum-Welch augmented with deter-
       ministic annealing.  Training algorithms can be run  multi-threaded  on
       hardware  with  multiple  cores/CPUs.   Forward,  backward, and Viterbi
       decoding are supported.  Automata for training/decoding are read from a
       text  file,  or  can  be  generated randomly or with uniform transition
       probabilities with different topologies (ergodic  or  fully  connected,
       Bakis  or  left-to-right,  or  deterministic).   Observations  used for
       training or decoding are read from text files.  The resulting automata,
       path  calculations, or probability calculations are printed to standard
       output.



</pre>
<h2>BASIC OPTIONS</h2><pre>
       The program has four main modes of operation: training (the  -T  flag),
       decoding  (the  -D  flag),  likelihood  calculation  (the -L flag), and
       sequence generation (the -G flag).  All modes except generation  depend
       on  an observations file and possibly a finite-state automaton file (if
       not initialized randomly for training).  The  observations  file  is  a
       text  file  and is assumed to consist of whitespace-separated sequences
       of integers, one observation sequence on each line.  Empty lines corre-
       spond  to  the empty string and are acceptable.  All output is directed
       to the standard output: when run in training mode, the output will con-
       sist  of a FSA in a text format (see below); when run in decoding mode,
       the output will be a string of whitespace-separated integers represent-
       ing  the  most  probable  path  through  a given FSA, one path for each
       observation; when run in likelihood calculation mode, the  output  will
       be one probability for each line in the observations file.


       -T bw|dabw|vit|vitbw
              Train  a  model with one of the four algorithms bw (Baum-Welch),
              dabw (Baum-Welch with  deterministic  annealing),  vit  (Viterbi
              training), or vitbw (Viterbi training followed by Baum-Welch).

       -D f|b|vit(,p)
              Decode  (find the best path through) the automaton for each word
              in obervation-file using either the forward path (  f  )  ,  the
              backward  path ( b ), or the Viterbi path ( vit ).  The optional
              ,p will also print out the respective probability together  with
              the  path.   Note that forward and backward decoding chooses the
              most probable state for each point in time and so the  path  may
              or  may  not correspond to an actually valid path in the automa-
              ton.  For example, -D vit,p will calculate the Viterbi path  and
              print its probability.

       -L f|vit
              Calculate  the  likelihood (probability) for each observation in
              observation-file using forward probability, or the Viterbi prob-
              ability.

       -G numsequences
              Generate  numsequences random sequences from FSA.  Randomness is
              weighted by transition probabilities.  The sequences are  output
              in three TAB-separated fields: (1) the sequence probability; (2)
              the symbol sequence itself; (3) the state sequence.


       -i input-format
              Set format of probabilities in input automata (real  numbers  or
              logs or negative logs in various bases), one of real, log10, ln,
              log2, nlog10, nln, nlog2.  Default is real.

       -o output-format
              Set format  of  probabilities  related  to  output  automata  or
              results of decoding and likelihood calculations (real numbers or
              logs or negative logs in various bases), one of real, log10, ln,
              log2, nlog10, nln, nlog2.  Default is real.

       -f fsm-file
              Specify finite state automaton file.  Each line in the automaton
              file consists of one to four numbers: a four-number line S1 S2 A
              P  indicates a transition from S1 to S2 with symbol A and proba-
              bility P whereas a line of the format S P  indicates  the  final
              probability  at  state S.  Three-number and one-number lines are
              identical to the above, with an implicit probabibility P of one.
              The initial state is always state number 0.  The format is iden-
              tical to the text formats accepted by the AT&T  FSM  toolkit  or
              OpenFST, with the exception that strings are not allowed to rep-
              resent symbols or states: all symbols  and  states  need  to  be
              integers.

              The  following  snippet  illustrates  a  typical FSA file of two
              states with an alphabet size of three using  real-valued  proba-
              bilities:


              0 0 0 0.25
             0 0 1 0.25
             0 1 0 0.2
             0 1 1 0.1
             0 1 2 0.1
             1 0 0 0.15
             1 0 1 0.15
             1 1 0 0.3
             1 1 1 0.1
             1 1 2 0.1
             0 0.1
             1 0.2



       -g type
              Generate  an  initial  automaton of type type for training.  The
              type is a combination of an option letter and the number of states and the alphabet of the format b|d|n#states(,#numsymbols)
              Here, b will generate a Bakis  (left-to-right)  automaton,  d  a
              deterministic  automaton, and n a fully connected (ergodic) non-
              deterministic automaton.  If numsymbols is omitted,  the  neces-
              sary  alphabet  size  will be deduced from the observation file.
              For example, -n20,10 will generate an  initial  fully  connected
              random automaton with 20 states and 10 symbols whereas -b10 will
              generate a Bakis automaton with 10 states, and the alphabet size
              is deduced from the largest symbol in the observations file.

       -u     This  option  sets  all initially generated automata with the -g
              option  to  have  uniform  probabilities  instead  of   randomly
              assigned ones.

       -h     print help and exit

       -v     print version and exit



</pre>
<h2>TRAINING OPTIONS</h2><pre>
       -d max-delta
              Maximum  delta (change in log likelihood between training itera-
              tions) to use for convergence.  Default is 0.1.  Note that treba
              will output the result of training calculations so far if CTRL-C
              is pressed without the need to wait for convergence.

       -x maxiterations
              Maximum number of iterations in training.   Default  is  100000.
              Note  that treba will output the result of training calculations
              so far if CTRL-C is pressed without the need to wait for conver-
              gence.

       -p pseudocounts
              Pseudocounts  to  use  in  Viterbi  training to prevent paths of
              probability zero. Default value is 1.

       -r restart,iterations-per-restart
              Sets Baum-Welch to restart itself  restart  times  running  for
              iterations-per-restart  iterations each restart.  After all ran-
              dom restarts have been run, the fsm with the best log likelihood
              is chosen and Baum-Welch proceeds as normal.


       -a betamin,betamax,alpha
              Controls  the  parameters  for  deterministic annealing when run
              with -T dabw setting the initial beta value to betamin, the max-
              imum  beta  value  to  betamax and the multiplier alpha by which
              beta is increased each time Baum-Welch converges.   The  default
              values are 0.02, 1.0, and 1.01.

       -t num-threads
              Number  of  threads to launch in Baum-Welch training.  The value
              num-threads can be optionally prefixed by c or  c/  to  use  c-n
              threads or c/n threads, where c is the number of logical CPUs on
              the system.  To use half  the  available  processors  one  would
              issue  the  flag -t c/2 and likewise -t c1 to use all but one of
              the available CPUs/cores. Default value is 1.


</pre>
<h2>EXAMPLE USAGE</h2><pre>
       treba -T bw -g n10 sentences.txt
              Reads all the sentences from sentences.txt and trains a 10-state
              probabilistic  automaton  using  Baum-Welch  using  the  default
              training parameters.  The initial automaton has random probabil-
              ities on its transitions and is fully connected.

       treba -T bw -t c/2 -g b100 sentences.txt
              Reads   all  the  sentences  from  sentences.txt  and  trains  a
              100-state probabilistic automaton using Baum-Welch.  The initial
              automaton is left-to-right.  During training, half the available
              CPUs will be used.

       treba -T dabw -t 8 -f initial.fsm sentences.txt
              Reads all the sentences from sentences.txt and trains  a  proba-
              bilistic  automaton  using Baum-Welch with deterministic anneal-
              ing.  The initial automaton is read from initial.fsm.   A  total
              of 8 threads will be launched in parallel for Baum-Welch.

       treba -T vit -g b25,5 -x 10 sentences.txt
              Reads all the sentences from sentences.txt and trains a 25-state
              probabilistic automaton with an alphabet size of 5 using Viterbi
              training running a maximum of 10 iterations.  The initial autom-
              aton is random and left-to-right (Bakis).

       treba -L f -f myfsm.fsm sentences.txt
              Reads sentences.txt and calculates for each observation line the
              forward probability in the automaton in myfsm.fsm.

       treba -D vit -f myfsm.fsm sentences.txt
              Reads sentences.txt and calculates for each observation line the
              most  probable  path  (the  Viterbi  path)  in   the   automaton
              myfsm.fsm.

       treba -D vit,p -f myfsm.fsm sentences.txt
              Reads sentences.txt and calculates for each observation line the
              most probable path through  the  automaton  in  myfsm.fsm.   The
              probability of the path is also printed.

       treba -G 100 -o log10 -f myfsm.fsm
              Generate 100 random sequences (weighted by transition probabili-
              ties) from myfsm.fsm.  Output probability scores are  log10  and
              myfsm.fsm has real-valued transitions (default).

       treba -i real -o nln -f myfsm.fsm
              No  real  action: myfsm.fsm is read (inputs are real-valued) and
              converted to negative logprobs (aka weights) with the base e and
              output  to  stdout.   This can be used to export an FSA for pro-
              cessing with e.g. the AT&T tools or OpenFST.  Not issuing any of
              the  flags  -T, -D or -L, will simply output the input FSA, per-
              forming a possible conversion depending on the input and  output
              specifiers.



</pre>
<h2>SEE ALSO</h2><pre>
       de  la Higuera, C. (2010). Grammatical Inference: Learning Automata and
       Grammars.  Cambridge University Press.

       Dempster, A., N. Laird, and D. Rubin. (1977). Maximum likelihood  esti-
       mation  from incomplete data via the EM algorithm. Journal of the Royal
       Statistical Society B, 39:1-38.

       Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and  selected
       applications  in speech recognition.  Proc. of the IEEE, 77(2):257-286.

       Rao, A. and K. Rose. (2001). Deterministically annealed design of  Hid-
       den  Markov  Model  speech recognizers. IEEE Transactions on Speech and
       Audio Processing, 9(2):111-126.

       Rose, K. (1998). Deterministic annealing for  clustering,  compression,
       classification, regression, and related optimization problems. Proc. of
       the IEEE, 86(11):2210-2239.

       Ueda, N. and Nakano, R. (1998). Deterministic annealing  EM  algorithm.
       Neural Networks, 11(2):271-282.

       Smith,  N.  A. and Eisner, J. (2004). Annealing techniques for unsuper-
       vised statistical  language  learning.  In  Proc.  of  the  ACL,  pages
       486-493.

       fsm(1)



</pre>
<h2>BUGS</h2><pre>
       Many and hairy. This is academic code. Don't build Mars rovers with it.



</pre>
<h2>AUTHOR</h2><pre>
       Mans Hulden <mans.hulden@gmail.com>



                                 July 9, 2012                         treba(1)
</pre>
<hr>